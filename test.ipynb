{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e9d93",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'multimodalrag (Python 3.12.0)' requires the ipykernel package.\n",
      "\u001b[1;31mInstall 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n multimodalrag ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "print(\"Hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6f24cfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! I'm just a computer program, so I don't have feelings, but I'm here to help you. How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "response = llm.invoke(\"Hi how are you\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8a8cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag.py\n",
    "\n",
    "import os\n",
    "import io\n",
    "import base64\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.schema.messages import HumanMessage\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Environment Setup\n",
    "# -----------------------------\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# CLIP Model Initialization\n",
    "# -----------------------------\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model.eval()\n",
    "\n",
    "BaseModel.model_config = {\"arbitrary_types_allowed\": True}\n",
    "\n",
    "\n",
    "def embed_image(image_data):\n",
    "    \"\"\"Embed image using CLIP\"\"\"\n",
    "    if isinstance(image_data, str):\n",
    "        image = Image.open(image_data).convert(\"RGB\")\n",
    "    else:\n",
    "        image = image_data\n",
    "    inputs = clip_processor(images=image, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_image_features(**inputs)\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.squeeze().numpy()\n",
    "\n",
    "\n",
    "def embed_text(text):\n",
    "    \"\"\"Embed text using CLIP\"\"\"\n",
    "    inputs = clip_processor(\n",
    "        text=text,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=77\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        features = clip_model.get_text_features(**inputs)\n",
    "        features = features / features.norm(dim=-1, keepdim=True)\n",
    "        return features.squeeze().numpy()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Multimodal RAG Pipeline Class\n",
    "# -----------------------------\n",
    "class MultimodalRAG:\n",
    "    def __init__(self):\n",
    "        self.docs = []\n",
    "        self.embeddings = []\n",
    "        self.image_data_store = {}\n",
    "        self.vector_store = None\n",
    "        self.llm = init_chat_model(\"openai:gpt-4.1\")\n",
    "        self.checkpointer = MemorySaver()\n",
    "        self.graph = self._create_graph()\n",
    "\n",
    "    def ingest_documents(self, docs):\n",
    "        \"\"\"Ingest a list of Document objects (text or image) into the RAG system.\"\"\"\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "        for doc in docs:\n",
    "            if doc.metadata.get(\"type\") == \"image\":\n",
    "                pil_image = doc.metadata[\"image\"]\n",
    "                image_id = doc.metadata[\"image_id\"]\n",
    "\n",
    "                buffered = io.BytesIO()\n",
    "                pil_image.save(buffered, format=\"PNG\")\n",
    "                img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
    "                self.image_data_store[image_id] = img_base64\n",
    "\n",
    "                emb = embed_image(pil_image)\n",
    "                self.embeddings.append(emb)\n",
    "                self.docs.append(doc)\n",
    "\n",
    "            else:  # text doc\n",
    "                text_chunks = splitter.split_documents([doc])\n",
    "                for chunk in text_chunks:\n",
    "                    emb = embed_text(chunk.page_content)\n",
    "                    self.embeddings.append(emb)\n",
    "                    self.docs.append(chunk)\n",
    "\n",
    "        # rebuild vector store\n",
    "        self.vector_store = FAISS.from_embeddings(\n",
    "            text_embeddings=[(doc.page_content, emb) for doc, emb in zip(self.docs, self.embeddings)],\n",
    "            embedding=None,\n",
    "            metadatas=[doc.metadata for doc in self.docs],\n",
    "        )\n",
    "\n",
    "    def _create_graph(self):\n",
    "        \"\"\"Define a simple retrieval ‚Üí generation graph.\"\"\"\n",
    "        workflow = StateGraph(dict)\n",
    "\n",
    "        def retrieve(state):\n",
    "            query = state[\"query\"]\n",
    "            query_emb = embed_text(query)\n",
    "            results = self.vector_store.similarity_search_by_vector(query_emb, k=5)\n",
    "            return {\"query\": query, \"context\": results}\n",
    "\n",
    "        def generate(state):\n",
    "            query, retrieved_docs = state[\"query\"], state[\"context\"]\n",
    "            content = [{\"type\": \"text\", \"text\": f\"Question: {query}\\n\\nContext:\\n\"}]\n",
    "\n",
    "            # Text docs\n",
    "            text_docs = [doc for doc in retrieved_docs if doc.metadata.get(\"type\") == \"text\"]\n",
    "            if text_docs:\n",
    "                text_context = \"\\n\\n\".join(\n",
    "                    [f\"[Source: {doc.metadata.get('source','')}] {doc.page_content}\" for doc in text_docs]\n",
    "                )\n",
    "                content.append({\"type\": \"text\", \"text\": f\"Text excerpts:\\n{text_context}\\n\"})\n",
    "\n",
    "            # Image docs\n",
    "            for doc in retrieved_docs:\n",
    "                if doc.metadata.get(\"type\") == \"image\":\n",
    "                    image_id = doc.metadata.get(\"image_id\")\n",
    "                    if image_id and image_id in self.image_data_store:\n",
    "                        content.append(\n",
    "                            {\"type\": \"text\", \"text\": f\"\\n[Image from {doc.metadata.get('source','')}]:\\n\"}\n",
    "                        )\n",
    "                        content.append(\n",
    "                            {\n",
    "                                \"type\": \"image_url\",\n",
    "                                \"image_url\": {\"url\": f\"data:image/png;base64,{self.image_data_store[image_id]}\"},\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "            content.append(\n",
    "                {\"type\": \"text\", \"text\": \"\\n\\nPlease answer the question based on the provided text and images.\"}\n",
    "            )\n",
    "\n",
    "            message = HumanMessage(content=content)\n",
    "            response = self.llm.invoke([message])\n",
    "            return {\"answer\": response.content}\n",
    "\n",
    "        workflow.add_node(\"retrieve\", retrieve)\n",
    "        workflow.add_node(\"generate\", generate)\n",
    "        workflow.set_entry_point(\"retrieve\")\n",
    "        workflow.add_edge(\"retrieve\", \"generate\")\n",
    "        workflow.add_edge(\"generate\", END)\n",
    "\n",
    "        return workflow.compile(checkpointer=self.checkpointer)\n",
    "\n",
    "    def query(self, query: str) -> str:\n",
    "        \"\"\"Run query through RAG pipeline with thread_id for memory.\"\"\"\n",
    "        if not self.vector_store:\n",
    "            return \"‚ö†Ô∏è No documents ingested yet. Please upload PDFs or images.\"\n",
    "        state = {\"query\": query}\n",
    "        result = self.graph.invoke(state, config={\"configurable\": {\"thread_id\": \"streamlit-session\"}})\n",
    "        return result.get(\"answer\", \"No answer found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85eaee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from PIL import Image\n",
    "from langchain_core.documents import Document\n",
    "from rag import MultimodalRAG   # corrected import\n",
    "\n",
    "st.set_page_config(page_title=\"üìö Multimodal RAG\", layout=\"wide\")\n",
    "\n",
    "# Initialize RAG system\n",
    "if \"rag\" not in st.session_state:\n",
    "    st.session_state[\"rag\"] = MultimodalRAG()\n",
    "rag = st.session_state[\"rag\"]\n",
    "\n",
    "# -------------------------------\n",
    "# Sidebar for ingestion\n",
    "# -------------------------------\n",
    "st.sidebar.title(\"üìÇ Data Ingestion\")\n",
    "\n",
    "st.sidebar.subheader(\"Upload PDFs\")\n",
    "uploaded_pdfs = st.sidebar.file_uploader(\n",
    "    \"Choose PDF files\", type=[\"pdf\"], accept_multiple_files=True\n",
    ")\n",
    "\n",
    "st.sidebar.subheader(\"Upload Images\")\n",
    "uploaded_images = st.sidebar.file_uploader(\n",
    "    \"Choose image files\", type=[\"png\", \"jpg\", \"jpeg\"], accept_multiple_files=True\n",
    ")\n",
    "\n",
    "# Ingest PDFs\n",
    "if uploaded_pdfs:\n",
    "    docs = []\n",
    "    with st.spinner(\"üìñ Processing PDFs...\"):\n",
    "        for uploaded_file in uploaded_pdfs:\n",
    "            pdf_reader = PdfReader(uploaded_file)\n",
    "            for page_num, page in enumerate(pdf_reader.pages):\n",
    "                text = page.extract_text()\n",
    "                if text:\n",
    "                    docs.append(\n",
    "                        Document(\n",
    "                            page_content=text,\n",
    "                            metadata={\n",
    "                                \"source\": uploaded_file.name,\n",
    "                                \"page\": page_num,\n",
    "                                \"type\": \"text\"\n",
    "                            },\n",
    "                        )\n",
    "                    )\n",
    "        if docs:\n",
    "            rag.ingest_documents(docs)\n",
    "            st.sidebar.success(f\"‚úÖ Ingested {len(docs)} text chunks from {len(uploaded_pdfs)} PDFs.\")\n",
    "\n",
    "# Ingest Images\n",
    "if uploaded_images:\n",
    "    image_docs = []\n",
    "    with st.spinner(\"üñºÔ∏è Processing images...\"):\n",
    "        for idx, uploaded_file in enumerate(uploaded_images):\n",
    "            try:\n",
    "                pil_image = Image.open(uploaded_file).convert(\"RGB\")\n",
    "                image_docs.append(\n",
    "                    Document(\n",
    "                        page_content=f\"[Image: {uploaded_file.name}]\",\n",
    "                        metadata={\n",
    "                            \"source\": uploaded_file.name,\n",
    "                            \"type\": \"image\",\n",
    "                            \"image_id\": f\"user_image_{idx}\",\n",
    "                            \"image\": pil_image,\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "            except Exception as e:\n",
    "                st.sidebar.error(f\"Error loading {uploaded_file.name}: {e}\")\n",
    "\n",
    "        if image_docs:\n",
    "            rag.ingest_documents(image_docs)\n",
    "            st.sidebar.success(f\"‚úÖ Ingested {len(image_docs)} images.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Main Column for Query\n",
    "# -------------------------------\n",
    "st.title(\"ü§ñ Multimodal RAG Assistant\")\n",
    "\n",
    "query = st.text_input(\"Ask a question about the uploaded documents & images:\")\n",
    "\n",
    "if query:\n",
    "    with st.spinner(\"üí° Thinking...\"):\n",
    "        answer = rag.query(query)\n",
    "    st.markdown(\"### üìå Answer\")\n",
    "    st.write(answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1948dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0905199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rag.py\n",
    "import os\n",
    "import io\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from pydantic import BaseModel\n",
    "from langchain_core.documents import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.schema.messages import HumanMessage\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Try to import faiss for fast similarity search; fallback to numpy\n",
    "try:\n",
    "    import faiss  # type: ignore\n",
    "    _FAISS_AVAILABLE = True\n",
    "except Exception:\n",
    "    _FAISS_AVAILABLE = False\n",
    "\n",
    "# -----------------------------\n",
    "# Environment Setup\n",
    "# -----------------------------\n",
    "load_dotenv()\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\", \"\")\n",
    "\n",
    "# -----------------------------\n",
    "# CLIP Model Initialization\n",
    "# -----------------------------\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
    "clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "clip_model.eval()\n",
    "\n",
    "# allow arbitrary types in pydantic models used by some libs\n",
    "BaseModel.model_config = {\"arbitrary_types_allowed\": True}\n",
    "\n",
    "# -----------------------------\n",
    "# Embedding Functions\n",
    "# -----------------------------\n",
    "@torch.no_grad()\n",
    "def embed_text(text: str) -> np.ndarray:\n",
    "    \"\"\"Return L2-normalized numpy vector for input text.\"\"\"\n",
    "    if text is None:\n",
    "        text = \"\"\n",
    "    inputs = clip_processor(text=[text], return_tensors=\"pt\", padding=True, truncation=True, max_length=77)\n",
    "    # move to device\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = v.to(device)\n",
    "    features = clip_model.get_text_features(**inputs)  # (1, dim)\n",
    "    features = features / features.norm(dim=-1, keepdim=True)\n",
    "    vec = features.cpu().numpy()[0].astype(np.float32)\n",
    "    return vec\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def embed_image(image: Image.Image) -> np.ndarray:\n",
    "    \"\"\"Return L2-normalized numpy vector for input PIL image.\"\"\"\n",
    "    inputs = clip_processor(images=[image], return_tensors=\"pt\")\n",
    "    for k, v in inputs.items():\n",
    "        inputs[k] = v.to(device)\n",
    "    features = clip_model.get_image_features(**inputs)  # (1, dim)\n",
    "    features = features / features.norm(dim=-1, keepdim=True)\n",
    "    vec = features.cpu().numpy()[0].astype(np.float32)\n",
    "    return vec\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Simple Vector Store (supports faiss if available)\n",
    "# -----------------------------\n",
    "class VectorStore:\n",
    "    def __init__(self):\n",
    "        self.embeddings = None  # numpy array shape (N, D)\n",
    "        self.metadatas = []     # list of dicts (length N)\n",
    "        self.documents = []     # list of Document (length N)\n",
    "        self._faiss_index = None\n",
    "        self.dimension = None\n",
    "\n",
    "    def add(self, emb: np.ndarray, doc: Document):\n",
    "        \"\"\"Add a single embedding + Document.\"\"\"\n",
    "        emb = np.asarray(emb, dtype=np.float32)\n",
    "        if emb.ndim == 1:\n",
    "            emb = emb.reshape(1, -1)\n",
    "        if self.embeddings is None:\n",
    "            self.embeddings = emb\n",
    "            self.dimension = emb.shape[1]\n",
    "        else:\n",
    "            # ensure same dim\n",
    "            if emb.shape[1] != self.dimension:\n",
    "                raise ValueError(\"Embedding dimension mismatch.\")\n",
    "            self.embeddings = np.vstack([self.embeddings, emb])\n",
    "        self.documents.append(doc)\n",
    "        self.metadatas.append(doc.metadata or {})\n",
    "\n",
    "    def build_index(self):\n",
    "        \"\"\"Build faiss index if available; otherwise keep embeddings for brute-force search.\"\"\"\n",
    "        if self.embeddings is None:\n",
    "            return\n",
    "        if _FAISS_AVAILABLE:\n",
    "            # Use inner product (embeddings are normalized => cosine similarity)\n",
    "            self._faiss_index = faiss.IndexFlatIP(self.dimension)\n",
    "            self._faiss_index.add(self.embeddings)\n",
    "        else:\n",
    "            self._faiss_index = None\n",
    "\n",
    "    def similarity_search_by_vector(self, query_emb: np.ndarray, k: int = 5):\n",
    "        \"\"\"\n",
    "        Return top-k Document objects sorted by similarity (highest first).\n",
    "        If fewer than k items exist, return all.\n",
    "        \"\"\"\n",
    "        if self.embeddings is None:\n",
    "            return []\n",
    "\n",
    "        q = np.asarray(query_emb, dtype=np.float32)\n",
    "        if q.ndim == 1:\n",
    "            q = q.reshape(1, -1)\n",
    "\n",
    "        if self._faiss_index is not None:\n",
    "            # faiss returns (distances, indices)\n",
    "            D, I = self._faiss_index.search(q, min(k, self.embeddings.shape[0]))\n",
    "            indices = I[0].tolist()\n",
    "            # distances are inner product scores\n",
    "            results = [self.documents[i] for i in indices if i != -1]\n",
    "            return results\n",
    "        else:\n",
    "            # brute-force cosine (since embeddings are normalized, cosine == dot)\n",
    "            sims = (self.embeddings @ q.T).squeeze(axis=1)  # (N,)\n",
    "            topk_idx = np.argsort(-sims)[:k]\n",
    "            results = [self.documents[i] for i in topk_idx]\n",
    "            return results\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Multimodal RAG Pipeline\n",
    "# -----------------------------\n",
    "class MultimodalRAG:\n",
    "    def __init__(self):\n",
    "        # keep text docs and image docs as Documents\n",
    "        self.vector_store = VectorStore()\n",
    "        # keep actual PIL images in memory keyed by source name for display\n",
    "        self.image_store = {}  # {source_name: PIL.Image}\n",
    "        # LLM\n",
    "        self.llm = init_chat_model(\"openai:gpt-4.1\")\n",
    "        # LangGraph checkpointer\n",
    "        self.checkpointer = MemorySaver()\n",
    "        # compiled graph\n",
    "        self.graph = self._create_graph()\n",
    "        # text splitter for PDFs/long text\n",
    "        self.splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "    # -------------------------\n",
    "    # Ingest Text Documents (PDF FAQ pages -> Document objects)\n",
    "    # -------------------------\n",
    "    def ingest_texts(self, docs):\n",
    "        \"\"\"\n",
    "        docs: list of langchain_core.documents.Document with page_content and metadata.\n",
    "        This method splits long text into chunks, computes embeddings and adds to vector store.\n",
    "        \"\"\"\n",
    "        for doc in docs:\n",
    "            # split into chunks\n",
    "            chunks = self.splitter.split_documents([doc])\n",
    "            for chunk in chunks:\n",
    "                text = chunk.page_content or \"\"\n",
    "                emb = embed_text(text)\n",
    "                # ensure chunk has metadata and a source\n",
    "                md = chunk.metadata or {}\n",
    "                md = dict(md)  # copy\n",
    "                md.setdefault(\"type\", \"text\")\n",
    "                chunk.metadata = md\n",
    "                self.vector_store.add(emb, Document(page_content=text, metadata=md))\n",
    "\n",
    "        # rebuild index after ingestion\n",
    "        self.vector_store.build_index()\n",
    "\n",
    "    # -------------------------\n",
    "    # Ingest Images\n",
    "    # -------------------------\n",
    "    def ingest_images(self, images: list):\n",
    "        \"\"\"\n",
    "        images: list of dicts like {\"image\": PIL.Image, \"name\": \"file.jpg\"}\n",
    "        Stores actual image objects in image_store and creates Document entries with placeholder text.\n",
    "        \"\"\"\n",
    "        for img_data in images:\n",
    "            img = img_data[\"image\"]\n",
    "            name = img_data.get(\"name\", \"image\")\n",
    "            # compute embedding\n",
    "            emb = embed_image(img)\n",
    "            # store image in memory for later display\n",
    "            self.image_store[name] = img.copy()\n",
    "            # create a document placeholder with metadata indicating image\n",
    "            md = {\"source\": name, \"type\": \"image\"}\n",
    "            doc = Document(page_content=f\"Image: {name}\", metadata=md)\n",
    "            self.vector_store.add(emb, doc)\n",
    "\n",
    "        # rebuild index after ingestion\n",
    "        self.vector_store.build_index()\n",
    "\n",
    "    # -------------------------\n",
    "    # Graph: Retrieval ‚Üí Generation\n",
    "    # -------------------------\n",
    "    def _create_graph(self):\n",
    "        workflow = StateGraph(dict)\n",
    "\n",
    "        def retrieve(state):\n",
    "            query_text = state.get(\"query_text\", \"\")\n",
    "            if not query_text:\n",
    "                return {\"query_text\": query_text, \"context\": []}\n",
    "\n",
    "            # embed the query using text encoder\n",
    "            query_emb = embed_text(query_text)\n",
    "            retrieved_docs = self.vector_store.similarity_search_by_vector(query_emb, k=8)  # fetch a few\n",
    "            return {\"query_text\": query_text, \"context\": retrieved_docs}\n",
    "\n",
    "        def generate(state):\n",
    "            query_text = state.get(\"query_text\", \"\")\n",
    "            retrieved_docs = state.get(\"context\", []) or []\n",
    "\n",
    "            # separate text and image docs\n",
    "            text_docs = [d for d in retrieved_docs if (d.metadata or {}).get(\"type\") == \"text\"]\n",
    "            image_docs = [d for d in retrieved_docs if (d.metadata or {}).get(\"type\") == \"image\"]\n",
    "\n",
    "            # prepare prompt/context for the LLM\n",
    "            context_blocks = []\n",
    "            if text_docs:\n",
    "                for d in text_docs:\n",
    "                    src = (d.metadata or {}).get(\"source\", \"\")\n",
    "                    context_blocks.append(f\"[Source: {src}] {d.page_content}\")\n",
    "\n",
    "            # build message content as the LLM wrapper expects\n",
    "            prompt_parts = [\n",
    "                {\"type\": \"text\", \"text\": f\"Question: {query_text}\\n\\n\"},\n",
    "            ]\n",
    "            if context_blocks:\n",
    "                prompt_parts.append({\"type\": \"text\", \"text\": \"Context from FAQs:\\n\" + \"\\n\\n\".join(context_blocks) + \"\\n\\n\"})\n",
    "            prompt_parts.append({\"type\": \"text\", \"text\": \"Please answer concisely and reference the sources (if any). If images are relevant, mention which image filenames are relevant.\"})\n",
    "\n",
    "            message = HumanMessage(content=prompt_parts)\n",
    "\n",
    "            # invoke LLM\n",
    "            try:\n",
    "                response = self.llm.invoke([message])\n",
    "            except Exception as e:\n",
    "                # graceful failure: return error as text\n",
    "                return {\"answer\": {\"text\": f\"LLM invocation failed: {e}\", \"images\": []}}\n",
    "\n",
    "            # robustly extract text from response\n",
    "            answer_text = \"\"\n",
    "            if isinstance(response, str):\n",
    "                answer_text = response\n",
    "            else:\n",
    "                # many LLM wrappers put the textual content in `.content`\n",
    "                answer_text = getattr(response, \"content\", None)\n",
    "                if answer_text is None:\n",
    "                    # fallback to string conversion\n",
    "                    answer_text = str(response)\n",
    "\n",
    "            # Prepare image objects to return (actual PIL images from image_store)\n",
    "            image_list = []\n",
    "            for img_doc in image_docs:\n",
    "                src = (img_doc.metadata or {}).get(\"source\")\n",
    "                if src and src in self.image_store:\n",
    "                    image_list.append({\"name\": src, \"image\": self.image_store[src]})\n",
    "\n",
    "            return {\"answer\": {\"text\": answer_text, \"images\": image_list}}\n",
    "\n",
    "        workflow.add_node(\"retrieve\", retrieve)\n",
    "        workflow.add_node(\"generate\", generate)\n",
    "        workflow.set_entry_point(\"retrieve\")\n",
    "        workflow.add_edge(\"retrieve\", \"generate\")\n",
    "        workflow.add_edge(\"generate\", END)\n",
    "\n",
    "        return workflow.compile(checkpointer=self.checkpointer)\n",
    "\n",
    "    # -------------------------\n",
    "    # Public Query\n",
    "    # -------------------------\n",
    "    def query(self, query_text: str):\n",
    "        if not query_text or not query_text.strip():\n",
    "            return {\"text\": \"Please provide a query.\", \"images\": []}\n",
    "\n",
    "        state = {\"query_text\": query_text}\n",
    "        result = self.graph.invoke(state, config={\"configurable\": {\"thread_id\": \"streamlit-session\"}})\n",
    "        # result is expected to be a dict from generate node\n",
    "        ans = result.get(\"answer\") if isinstance(result, dict) else result\n",
    "        if not ans:\n",
    "            return {\"text\": \"No answer found.\", \"images\": []}\n",
    "        text = ans.get(\"text\", \"\")\n",
    "        images = ans.get(\"images\", [])\n",
    "        return {\"text\": text, \"images\": images}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed91dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py\n",
    "import streamlit as st\n",
    "from PyPDF2 import PdfReader\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from langchain_core.documents import Document\n",
    "from rag import MultimodalRAG\n",
    "\n",
    "st.set_page_config(page_title=\"E-commerce Product Assistant\", layout=\"wide\")\n",
    "\n",
    "st.title(\"üõí E-commerce Product FAQ & Image Assistant\")\n",
    "\n",
    "# Initialize or reuse rag in session state so we don't lose data\n",
    "if \"rag\" not in st.session_state:\n",
    "    st.session_state[\"rag\"] = MultimodalRAG()\n",
    "rag: MultimodalRAG = st.session_state[\"rag\"]\n",
    "\n",
    "# -----------------------------\n",
    "# Sidebar: Document & Image Ingestion\n",
    "# -----------------------------\n",
    "st.sidebar.header(\"üìÇ Upload FAQs and Product Images\")\n",
    "\n",
    "pdf_files = st.sidebar.file_uploader(\"Upload FAQ PDFs\", type=[\"pdf\"], accept_multiple_files=True)\n",
    "image_files = st.sidebar.file_uploader(\"Upload Product Images\", type=[\"png\", \"jpg\", \"jpeg\"], accept_multiple_files=True)\n",
    "\n",
    "process_button = st.sidebar.button(\"Process / Add to Index\")\n",
    "\n",
    "if process_button:\n",
    "    # Process PDFs (text)\n",
    "    if pdf_files:\n",
    "        docs = []\n",
    "        with st.spinner(\"üìñ Processing PDFs...\"):\n",
    "            for uploaded_file in pdf_files:\n",
    "                try:\n",
    "                    pdf_reader = PdfReader(uploaded_file)\n",
    "                    for page_num, page in enumerate(pdf_reader.pages):\n",
    "                        text = page.extract_text()\n",
    "                        if text and text.strip():\n",
    "                            docs.append(\n",
    "                                Document(\n",
    "                                    page_content=text,\n",
    "                                    metadata={\"source\": uploaded_file.name, \"page\": page_num, \"type\": \"text\"},\n",
    "                                )\n",
    "                            )\n",
    "                except Exception as e:\n",
    "                    st.sidebar.error(f\"Failed to read {uploaded_file.name}: {e}\")\n",
    "\n",
    "        if docs:\n",
    "            rag.ingest_texts(docs)\n",
    "            st.sidebar.success(f\"‚úÖ Ingested {len(docs)} text chunks from {len(pdf_files)} PDFs.\")\n",
    "        else:\n",
    "            st.sidebar.info(\"No text chunks found in uploaded PDFs.\")\n",
    "\n",
    "    # Process Images\n",
    "    if image_files:\n",
    "        images = []\n",
    "        with st.spinner(\"üñºÔ∏è Processing Images...\"):\n",
    "            for uploaded_file in image_files:\n",
    "                try:\n",
    "                    # Read bytes and make PIL image\n",
    "                    img = Image.open(uploaded_file).convert(\"RGB\")\n",
    "                    images.append({\"image\": img, \"name\": uploaded_file.name})\n",
    "                except Exception as e:\n",
    "                    st.sidebar.error(f\"Failed to read image {uploaded_file.name}: {e}\")\n",
    "\n",
    "        if images:\n",
    "            rag.ingest_images(images)\n",
    "            st.sidebar.success(f\"‚úÖ Ingested {len(images)} images.\")\n",
    "        else:\n",
    "            st.sidebar.info(\"No images ingested.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Index Info (optional)\n",
    "# -----------------------------\n",
    "st.sidebar.markdown(\"---\")\n",
    "st.sidebar.write(\"Index info:\")\n",
    "num_items = 0\n",
    "if rag.vector_store.embeddings is not None:\n",
    "    num_items = rag.vector_store.embeddings.shape[0]\n",
    "st.sidebar.write(f\"Items indexed: **{num_items}**\")\n",
    "st.sidebar.write(f\"Images stored: **{len(rag.image_store)}**\")\n",
    "\n",
    "# -----------------------------\n",
    "# Main: Query\n",
    "# -----------------------------\n",
    "st.subheader(\"üí¨ Ask a Question about Products\")\n",
    "\n",
    "query_text = st.text_area(\"Enter your question:\", height=150)\n",
    "\n",
    "if st.button(\"üîé Search\"):\n",
    "    if rag.vector_store.embeddings is None:\n",
    "        st.error(\"‚ö†Ô∏è Please process FAQs and images first from the sidebar (click 'Process / Add to Index').\")\n",
    "    elif not query_text or not query_text.strip():\n",
    "        st.warning(\"Please enter a query.\")\n",
    "    else:\n",
    "        with st.spinner(\"üí° Thinking...\"):\n",
    "            answer = rag.query(query_text)\n",
    "\n",
    "        st.markdown(\"### üìë Answer\")\n",
    "        st.write(answer[\"text\"])\n",
    "\n",
    "        if answer.get(\"images\"):\n",
    "            st.markdown(\"### üñºÔ∏è Relevant Images\")\n",
    "            cols = st.columns(3)\n",
    "            for i, img_obj in enumerate(answer[\"images\"]):\n",
    "                # img_obj is {\"name\": name, \"image\": PIL.Image}\n",
    "                name = img_obj.get(\"name\", f\"image_{i}\")\n",
    "                img = img_obj.get(\"image\")\n",
    "                if img is not None:\n",
    "                    col = cols[i % 3]\n",
    "                    col.image(img, caption=name, use_column_width=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodalrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
